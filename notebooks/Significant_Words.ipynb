{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "million-infection",
   "metadata": {},
   "source": [
    "# Significant words\n",
    "\n",
    "https://moodle2.uni-leipzig.de/pluginfile.php/2889299/mod_resource/content/0/distant_readingII.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "planned-brand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact match shapes (1819808, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#df = pd.read_parquet('sources/metadata.parquet', engine='pyarrow')\n",
    "#df_large = pd.read_parquet('sources/metadata-large.parquet', engine='pyarrow')\n",
    "\n",
    "exact_matches = pd.read_parquet(\"../results/artists_exact_match_large.parquet\", engine='pyarrow')\n",
    "assert \"artists\" in exact_matches.columns, f'artists is not in {exact_matches.columns}'\n",
    "assert \"num_artists\" in exact_matches.columns, f'num_artists is not in {exact_matches.columns}'\n",
    "\n",
    "print(f'exact match shapes {exact_matches.shape}')\n",
    "\n",
    "# read excel_artist_names\n",
    "import my_utils\n",
    "\n",
    "excel_artist_names = my_utils.read_lines_as_list(\"../sources/excel_artists_copy_paste_name.txt\")\n",
    "hundred_artist_names = excel_artist_names[0:100]\n",
    "ten_artist_names = excel_artist_names[0:10]\n",
    "\n",
    "\n",
    "artist_mentions = pd.read_parquet('../results/artist_mentions.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "about-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def read_stopwords():\n",
    "    # from https://gist.github.com/sebleier/554280\n",
    "    return my_utils.read_lines_as_list(\"stopwords.txt\")\n",
    "\n",
    "def compute_corpus(df, col):\n",
    "    # print(sw)\n",
    "    corpus = []\n",
    "    for i, row in df.iterrows():\n",
    "        string = row[col]\n",
    "        words = []\n",
    "        for s in re.split(\"[, \\-!?:]+\", string):\n",
    "            s=s.strip().lower()\n",
    "            if len(s)>0:\n",
    "                words.append(s)\n",
    "        corpus.append(\" \".join(words))\n",
    "    return corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "mental-right",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1819808\n",
      "beautiful porcelain ivory fair face woman biomechanical cyborg, close - up, sharp focus, studio light, iris van herpen haute couture headdress made of rhizomorphs, daisies, brackets, colorful corals, fractal mushrooms, puffballs, octane render, ultra sharp, 8 k \n"
     ]
    }
   ],
   "source": [
    "corpus = exact_matches.prompt.to_list()\n",
    "print(len(corpus))\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "editorial-experience",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = read_stopwords()\n",
    "\n",
    "#corpus = compute_corpus(exact_matches, \"prompt\")\n",
    "#print(len(corpus))\n",
    "#print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "#greg_df = my_utils.exact_match_dataframe(exact_matches, \"greg rutkowski\")\n",
    "#freqs_greg = compute_freq(greg_df, \"prompt\")\n",
    "#freqs_greg = remove(freqs_greg, [\"greg\", \"rutkowski\"])\n",
    "#compute_wc(freqs_greg, \"greg.png\", top)\n",
    "#print(f'finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "becoming-florist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1819808 documents\n",
      "171528 words\n",
      "['00' '000' '0000' ... 'ð™ð™§ð™žð™šð™™ð™§ð™žð™˜ð™' 'ðŸ–ð–' 'ðŸ­ðŸ´ðŸ¬ðŸ¬']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#corpus = [\n",
    "#    'This is the first document.',\n",
    "#    'This document is the second document.',\n",
    "#    'And this is the third one.',\n",
    "#    'Is this the first document?',\n",
    "#]\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f'{X.shape[0]} documents')\n",
    "print(f'{X.shape[1]} words')\n",
    "print(feature_names)\n",
    "#print(X)\n",
    "#print(f'Stopwords {vectorizer.get_stop_words()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-sixth",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(vectorizer.inverse_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dumb way takes ages!!!\n",
    "#mean_tf_idf = {}\n",
    "#for index, word in enumerate(feature_names):\n",
    "    #print(word)\n",
    "#    idf_sum=0\n",
    "#    for doc in range(X.shape[0]):\n",
    "#        #print(f'X[{doc}, {index}] is {X[doc, index]}')\n",
    "#        idf_sum+=X[doc, index]\n",
    "#    #print(f'{word} {idf_sum/X.shape[0]}')\n",
    "#    mean_tf_idf[word] = idf_sum/X.shape[0]\n",
    "#    \n",
    "#    print(f'{index} {word} {mean_tf_idf[word]}')\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mean_tf_idfs = X.mean(axis=0)\n",
    "print(mean_tf_idfs.shape)\n",
    "mapping={}\n",
    "print(mean_tf_idfs[0,0])\n",
    "for index, fn in enumerate(feature_names):\n",
    "    mapping[fn]=mean_tf_idfs[0,index]\n",
    "\n",
    "df = pd.DataFrame.from_dict(mapping, orient=\"index\", columns = [\"tf_idf\"])\n",
    "\n",
    "df.sort_values(\"tf_idf\", axis=0, ascending=False, inplace=True)\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-blade",
   "metadata": {},
   "source": [
    "# the average tf*idf is not computed over the non_zero values\n",
    "\n",
    "When computing only over non_zero weights, we get total garbage (low frequency words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "environmental-black",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1819808, 171528)\n",
      "[[21.84440181 42.78653464  0.69930387 ...  0.38929793  0.14586499\n",
      "   0.21821789]]\n",
      "(1, 171528)\n",
      "21.844401810916068\n",
      "(array([      0,       0,       0, ..., 1819807, 1819807, 1819807],\n",
      "      dtype=int32), array([156181, 125509, 108202, ..., 111591,  61803, 144632], dtype=int32))\n",
      "(30767498,)\n",
      "(30767498,)\n",
      "should be 62: 62\n",
      "(1, 171528)\n",
      "0.3523290614663882\n",
      "                 tf_idf  query_mentions\n",
      "mauvey              1.0               1\n",
      "solastalgia         1.0               2\n",
      "gababooboobabba     1.0               1\n",
      "gababooba           1.0               1\n",
      "schnÃ¤bi             1.0               1\n",
      "          tf_idf  query_mentions\n",
      "dog     0.307528           11868\n",
      "trump   0.306534           11084\n",
      "car     0.277406           11355\n",
      "emma    0.273945           10250\n",
      "riding  0.267862           10896\n"
     ]
    }
   ],
   "source": [
    "# mean of non-zero\n",
    "import numpy as np\n",
    "print(X.shape)\n",
    "sums = np.sum(X, axis=0)\n",
    "print(sums)\n",
    "print(sums.shape)\n",
    "print(sums[0, 0])\n",
    "\n",
    "nonzero_mean = sums\n",
    "\n",
    "from collections import defaultdict\n",
    "print(X.nonzero())\n",
    "print(X.nonzero()[0].shape)\n",
    "print(X.nonzero()[1].shape)\n",
    "word_weight_indices = X.nonzero()[1]\n",
    "word_occurences = defaultdict(lambda: 0)\n",
    "for w in word_weight_indices:\n",
    "    word_occurences[w]=word_occurences[w]+1\n",
    "\n",
    "for index in range (nonzero_mean.shape[1]): #iterate over the elements\n",
    "    amount_nonzero = word_occurences[index]\n",
    "    #X.getcol(index).nonzero()[0].shape[0]\n",
    "    if index==0:\n",
    "        print(f'should be 62: {amount_nonzero}')\n",
    "        assert amount_nonzero == 62\n",
    "    tf_idf_sum = sums[0, index]\n",
    "    nonzero_mean[0, index] = tf_idf_sum / amount_nonzero\n",
    "    #if tf_idf_sum / amount_nonzero == 1:\n",
    "    #    print(f'div is one for {amount_nonzero} mentions')\n",
    "\n",
    "\n",
    "mean_tf_idfs = nonzero_mean\n",
    "print(mean_tf_idfs.shape)\n",
    "mapping={}\n",
    "print(mean_tf_idfs[0,0])\n",
    "for index, fn in enumerate(feature_names):\n",
    "    mapping[fn]=[mean_tf_idfs[0,index], word_occurences[index]]\n",
    "\n",
    "df = pd.DataFrame.from_dict(mapping, orient=\"index\", columns = [\"tf_idf\", \"query_mentions\"])\n",
    "\n",
    "df.sort_values(\"tf_idf\", axis=0, ascending=False, inplace=True)\n",
    "print(df.head())\n",
    "print(df[df[\"query_mentions\"]>100].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "expected-limit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0, 1, 1, 2, 2, 3, 3], dtype=int32), array([0, 1, 3, 0, 2, 4, 0, 1], dtype=int32))\n",
      "(8,)\n",
      "(8,)\n",
      "finished\n",
      "defaultdict(<function <lambda> at 0x148039518790>, {0: 3, 1: 2, 3: 1, 2: 1, 4: 1})\n",
      "(array([0, 1, 3], dtype=int32), array([0, 0, 0], dtype=int32))\n",
      "(3,)\n",
      "(3,)\n",
      "(4, 1)\n",
      "  (0, 0)\t0.6292275146695526\n",
      "  (1, 0)\t0.78722297610404\n",
      "  (3, 0)\t0.6292275146695526\n",
      "[array(['document', 'first'], dtype='<U8'), array(['second', 'document'], dtype='<U8'), array(['one', 'third'], dtype='<U8'), array(['document', 'first'], dtype='<U8')]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m inverse \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39minverse_transform(X)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(inverse)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43minverse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "print(X.nonzero())\n",
    "print(X.nonzero()[0].shape)\n",
    "print(X.nonzero()[1].shape)\n",
    "word_weight_indices = X.nonzero()[1]\n",
    "word_occurences = defaultdict(lambda: 0)\n",
    "for w in word_weight_indices:\n",
    "    word_occurences[w]=word_occurences[w]+1\n",
    "print('finished')\n",
    "print(word_occurences)\n",
    "    \n",
    "    \n",
    "col0=X.getcol(0)\n",
    "nz = col0.nonzero()\n",
    "print(nz)\n",
    "print(nz[0].shape)\n",
    "print(nz[1].shape)\n",
    "print(col0.shape)\n",
    "print(col0)\n",
    "\n",
    "\n",
    "\n",
    "inverse = vectorizer.inverse_transform(X)\n",
    "print(inverse)\n",
    "print(inverse.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-belfast",
   "metadata": {},
   "source": [
    "# Check if results are really different from frequency ranking\n",
    "\n",
    "Yes they are different but not by much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "def read_stopwords():\n",
    "    # from https://gist.github.com/sebleier/554280\n",
    "    return my_utils.read_lines_as_list(\"stopwords.txt\")\n",
    "\n",
    "def compute_freq(df, col):\n",
    "    sw = read_stopwords()\n",
    "    # print(sw)\n",
    "    freq = defaultdict(lambda: 0)\n",
    "    for i, row in df.iterrows():\n",
    "        string = row[col]\n",
    "        for s in re.split(\"[, \\-!?:]+\", string):\n",
    "            s=s.strip().lower()\n",
    "            if s not in sw:\n",
    "                if len(s)>0:\n",
    "                    freq[s]=freq[s]+1\n",
    "    return freq\n",
    "    \n",
    "def remove(freqs, words):\n",
    "    \n",
    "    result = defaultdict(lambda: 0)\n",
    "    for key, value in freqs.items():\n",
    "        #print(f'key {key}, value {value}')\n",
    "        if key not in words:\n",
    "            result[key]=value\n",
    "    return result\n",
    "\n",
    "freqs = compute_freq(exact_matches, \"prompt\")\n",
    "freqs = remove(freqs, [\"greg\", \"rutkowski\"])\n",
    "print(f'finished')\n",
    "\n",
    "freqs_df = pd.DataFrame.from_dict(freqs, orient=\"index\", columns = [\"counts_plain\"])\n",
    "freqs_df.index.name = 'word_plain'\n",
    "\n",
    "freqs_df.sort_values(\"counts_plain\", axis=0, ascending=False, inplace=True)\n",
    "#freqs_df['length']=freqs_df.word_plain.apply(lambda x: len(x))\n",
    "freqs_df.reset_index(inplace=True)\n",
    "freqs_sum = freqs_df.counts_plain.sum()\n",
    "freqs_df['frequency_plain']=freqs_df.counts_plain.apply(lambda x: x / freqs_sum)\n",
    "\n",
    "print(freqs_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-object",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my env conda",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
